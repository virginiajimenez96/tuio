{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae40f11",
   "metadata": {},
   "source": [
    "## Exercise 4: External Data Integration & Business Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b7d99476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "from meteostat import Point, Daily\n",
    "from geopy.geocoders import Nominatim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344718ab",
   "metadata": {},
   "source": [
    "##### 0. Informe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7cecf5",
   "metadata": {},
   "source": [
    "**Estrategia**\n",
    "\n",
    "Como compañía de seguros, el objetivo es tener la mayor cantidad de pólizas al mejor precio posible mientras minimizamos la cantidad de reclamaciones. Debido a esto, podemos analizar dos factores: \n",
    "\n",
    "- cómo de probable es que un cliente realice alguna acción que provoque una reclamación\n",
    "- qué tan probable es que un factor externo cause un problema que derive en una reclamación\n",
    "\n",
    "Ya que inferir resultados basados en condiciones personales podría ser difícil o disriminatorio, vamos a enfocarnos en factores externos.\n",
    "\n",
    "**Dataset elegido**\n",
    "\n",
    "Los datos con los que estamos trabajando continene información variada de cada cliente (ciudad, profesión, edad) pero la cantidad de datos es limitada. Un análisis interesante puede ser agrupar a los clientes por ciudad aunque resultaría en un análisis muy genérico. Por eso, podemos considerar agregar datos externos como los datos meteorológicos históricos de cada una de las ciudades en las que los clientes han realizado una reclamación. \n",
    "\n",
    "Además, el clima afecta a todas las pólizas ofrecidas (salud, automóviles, hogar...) por lo que parece un buen punto de partida.\n",
    "\n",
    "**Cómo ayudaría a la empresa**\n",
    "\n",
    "- Predecir qué clientes son más propensos a realizar reclamaciones en función del clima de su zona.\n",
    "- Ayuda a ajustar las pólizas en regiones con condiciones climáticas de riesgo.\n",
    "- Envío de alertas personalizadas a clientes según el histórico del clima, por ejemplo, \"se acerca temporada de muchaslluvias, revisa el estado de tu tejado\".\n",
    "- Ofrecer productos adicionales según el clima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62802f5",
   "metadata": {},
   "source": [
    "##### 1. Connect to APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f442f9",
   "metadata": {},
   "source": [
    "Let's collect the weather data 7 days prior to every claim in the corresponding city.\n",
    "\n",
    "We will use geopy for location data and meteostat for weather data.\n",
    "\n",
    "Geopy has a free API, just needing registration in https://www.geonames.org/\n",
    "\n",
    "Meteostat has a free API with no login required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "48b954bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to geopy\n",
    "geolocator = Nominatim(user_agent=\"vjm1996\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74d27b",
   "metadata": {},
   "source": [
    "##### 2. Load data from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68c9f81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to PostgreSQL...\n",
      "Data read from DB!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"../config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "    db_name = config[\"DB_NAME\"]\n",
    "    db_user = config[\"DB_USER\"]\n",
    "    db_password = config[\"DB_PASSWORD\"]\n",
    "    db_host = config[\"DB_HOST\"]\n",
    "    db_port = int(config[\"DB_PORT\"])\n",
    "\n",
    "DB_URL = f\"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "customers = pd.read_sql(\"SELECT * FROM customers\", con=engine)\n",
    "policies = pd.read_sql(\"SELECT * FROM policies\", con=engine)\n",
    "claims = pd.read_sql(\"SELECT * FROM claims\", con=engine)\n",
    "risk_indicators = pd.read_sql(\"SELECT * FROM risk_indicators\", con=engine)\n",
    "print(\"Data read from DB!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3de2e5",
   "metadata": {},
   "source": [
    "##### 3. Collecting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ebbbb0",
   "metadata": {},
   "source": [
    "First, finding out which cities and dates to collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "987dea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing those with missing date (2020-01-01 by default)\n",
    "claims_filtered = claims[claims['claim_date'] != '2020-01-01']\n",
    "\n",
    "# Left claims with customer cities\n",
    "claims_merged_cities = pd.merge(claims_filtered, customers[['customer_id','state']], on='customer_id', how='left')\n",
    "\n",
    "# Removing duplicate data for city and date\n",
    "unique_data = claims_merged_cities[['claim_date','state']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c43f30",
   "metadata": {},
   "source": [
    "Then querying APIs to collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "56bfc659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding weather data for león -> 2020-07-31 00:00:00\n",
      "Finding weather data for león -> 2023-01-04 00:00:00\n",
      "Finding weather data for león -> 2022-12-26 00:00:00\n",
      "Finding weather data for huesca -> 2022-05-10 00:00:00\n",
      "Finding weather data for vizcaya -> 2024-05-18 00:00:00\n",
      "Finding weather data for vizcaya -> 2022-05-01 00:00:00\n",
      "Finding weather data for vizcaya -> 2022-06-25 00:00:00\n",
      "Finding weather data for vizcaya -> 2022-01-18 00:00:00\n",
      "Finding weather data for vizcaya -> 2021-12-29 00:00:00\n",
      "Finding weather data for vizcaya -> 2022-05-07 00:00:00\n",
      "Finding weather data for vizcaya -> 2024-08-02 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Since APIs probably have rate limits, it is better to use a for loop\n",
    "# Otherwise, a concurrent approach could be chosen.\n",
    "\n",
    "# For a small loop, all weather will be stored in a dataframe. For a bigger loop, data could be stored directly in DB\n",
    "all_weather = pd.DataFrame({})\n",
    "\n",
    "for index, row in unique_data.iterrows():\n",
    "\n",
    "    print(\"Finding weather data for \" + row['state'] + \" -> \" + str(row['claim_date']))\n",
    "    location = geolocator.geocode(row['state'] + \" Spain\")\n",
    "\n",
    "    # Set time period (one week to also collect incidents during a week)\n",
    "    start = row['claim_date'] - pd.Timedelta(days=7)\n",
    "    end = row['claim_date']\n",
    "\n",
    "    # Create Point for city\n",
    "    city = Point(location.latitude, location.longitude)\n",
    "\n",
    "    # Get daily data - weather for each day\n",
    "    data = Daily(city, start, end)\n",
    "    data = data.fetch()\n",
    "\n",
    "    # Add city and date to weather data\n",
    "    data['state'] = row['state']\n",
    "    data.reset_index(inplace=True)\n",
    "    \n",
    "    # Adding data to general table\n",
    "    all_weather = pd.concat([all_weather, data], ignore_index=True)\n",
    "\n",
    "    # Sleeping to not overload API\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # Exiting after 10 cities to not overload API - just for testing\n",
    "    if index >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c622bdf9",
   "metadata": {},
   "source": [
    "#### 4. Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7ae36b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time      0\n",
       "tavg      0\n",
       "tmin      0\n",
       "tmax      0\n",
       "prcp      0\n",
       "snow     88\n",
       "wdir      0\n",
       "wspd      0\n",
       "wpgt      0\n",
       "pres      0\n",
       "tsun     88\n",
       "state     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309305e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weather.drop_duplicates(inplace=True)\n",
    "all_weather.fillna(all_weather.mean(numeric_only=True), inplace=True)\n",
    "cleaned_df = all_weather.drop(columns=['snow', 'tsun'])\n",
    "cleaned_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c65dd",
   "metadata": {},
   "source": [
    "#### 5. Storing data in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2ae3ec9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.to_sql(\"historical_weather\", con=engine, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae946ae8",
   "metadata": {},
   "source": [
    "#### 6. Merging weather data to claims df (only specific day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ef4feeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_with_weather = pd.merge(claims_merged_cities,all_weather,left_on=[\"claim_date\",\"state\"],right_on=[\"time\",\"state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c697d9c",
   "metadata": {},
   "source": [
    "Now analysis could continue, investigating if most claims happen during bad weather or any other hypotheses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
